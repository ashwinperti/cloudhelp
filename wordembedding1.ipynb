{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wordembedding1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPGfZux6HyMKfh2xuvJ/DK1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashwinperti/cloudhelp/blob/gh-pages/wordembedding1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NV3tt5V-75fd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "651f090b-ff87-4bbb-96c5-91da67fd9c29"
      },
      "source": [
        "from numpy import array\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.embeddings import Embedding"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fU4sFTK8Cdc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = [\n",
        "    # Positive Reviews\n",
        "\n",
        "    'This is an excellent movie',\n",
        "    'The move was fantastic I like it',\n",
        "    'You should watch it is brilliant',\n",
        "    'Exceptionally good',\n",
        "    'Wonderfully directed and executed I like it',\n",
        "    'Its a fantastic series',\n",
        "    'Never watched such a brillent movie',\n",
        "    'It is a Wonderful movie',\n",
        "\n",
        "    # Negtive Reviews\n",
        "\n",
        "    \"horrible acting\",\n",
        "    'waste of money',\n",
        "    'pathetic picture',\n",
        "    'It was very boring',\n",
        "    'I did not like the movie',\n",
        "    'The movie was horrible',\n",
        "    'I will not recommend',\n",
        "    'The acting is pathetic'\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnEIY6Jv8Hdk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Corpus has 8 positive reviews and 8 negative reviews. The next step is to create label set for our data.\n",
        "sentiments = array([1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0])\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "all_words = []\n",
        "for sent in corpus:\n",
        "    tokenize_word = word_tokenize(sent)\n",
        "    for word in tokenize_word:\n",
        "        all_words.append(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7wjLn8J9O7j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "68f6d041-781a-4db8-f911-feb48a612f70"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e03QAK3v8ItQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "91760213-69c5-4d24-d669-d549be05d901"
      },
      "source": [
        "unique_words = set(all_words)\n",
        "print(len(unique_words))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "45\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5nmT1Wl9bY_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "a1845735-cebd-4d19-c777-8ccfd2636380"
      },
      "source": [
        "vocab_length = 50\n",
        "embedded_sentences = [one_hot(sent, vocab_length) for sent in corpus]\n",
        "print(embedded_sentences )"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[43, 20, 41, 11, 9], [11, 1, 49, 41, 26, 29, 17], [45, 21, 31, 17, 20, 6], [43, 20], [1, 42, 15, 30, 26, 29, 17], [42, 16, 41, 22], [27, 36, 31, 16, 45, 9], [17, 20, 16, 39, 9], [16, 24], [43, 10, 27], [14, 12], [17, 49, 39, 8], [26, 23, 36, 29, 11, 9], [11, 9, 49, 16], [26, 8, 36, 1], [11, 24, 20, 14]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}